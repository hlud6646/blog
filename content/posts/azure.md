+++
title = 'Bookstore Analytics with Azure Data Factory'
date = 2025-02-08T13:21:39+11:00
draft = true
tags = ["reference", "Azure", "SQL Server"]
+++

This project should be a simple exploration of using **Data Factory**
for some toy analytics.  We will create a factory, and connect to several
**linked services** (i.e. other places where data could be stored), run
some **transformations** on incoming data and write to an **SQL server**
to record the results. This is more "how to navigate and use Azure" than
"how to do complicated ETL." It's easy enough to load a CSV file into a
database locally, but when you throw in 'system assigned managed identities',
'AutoResolveIntegrationRuntime' and the azure GUI things get complicated.


## 1. Create a Data Factory
For now let's think of a *data factory* as an ETL pipeline.
It's easy enough to create one in the DF online studio.
This one is called **bookstore-analytics**.

## 2. Link the Sources and Sink

### 2.1 Book Catalogue (HTTP Connector)
There is publicly available catalogue of books that we can use as a starting
point for the analysis. It is available at
[https://github.com/hlud6646/example-data/blob/main/books.csv](https://github.com/hlud6646/example-data/blob/main/books.csv).
The first task is to create a connection to this file, via the HTTP connector.
In the azure portal this is under **Manage -> Linked Services -> Create -> HTTP**.
For simplicity we can use anonymous authentication (since the file is public) but
this is probably going to be a rare configuration.

### 2.2 Book Sales (Azure File Storage Connector)
A retailor that we are working with has given us access to their internal
data platform. They use an **Azure file share** and a highly redundant data
model where a customer's entire profile is stored in each order they make.
As before we only need to create a linked service though the portal.

### 2.3 Analytics Database (SQL)
The goal of this pipeline is to populate an **SQL database** that the
analysts can use. So we create one called *bookstore-analytics* on an SQL server called
*bookstore-project*. I think the easist authentication to configure will
be plain old sql auth, so we'll go with that.

## 3. Ingest Data
For the first pass will aim to simply move the data from the sources to the
database. The **Ingest** guides you through this and is easy enough.
Importantly, this does not allow you to do any transformations (like creating
a proxy for a customer's name, or coercing data to its proper types) so everything
is just dumped into a new table in the database where every column is nvarchar.
 It's a start though.

## 4. Transform with Data Flow
We need to apply some transformations to the data before writing to the database.
For the catalogue, we should coerce numerical fields (PageCount, rating etc) to
a proper type. This will be handled in a **Data flow**, essentially a code free
data transformation solution.

### 4.1 Create Datasets
When we used the automated workflow earlier this part was done for us. We could
resuse the autogenerated artifacts but they have cryptic names, so we'll make
new ones.
In **Author -> Datasets** we should create a new one called *SalesCSV* and
configure it to read the `sales.csv` file that the retailer maintains.
[Do the same for catalogue]

## Impressions
- Very generally, I thought I was a code over GUI person. I've actually found the
graph layout of ETL steps very satisfying.

## Gripes
- To preview data while creating a data flow you need to start a *debug session*, which takes about a thousand
years to launch.
- Data flow lets you create a dataset from a connection to AzureFileStorage. But if you try to use this dataset
in a data flow you get a maddening error:
> Dataset is using 'AzureFileStorage' linked service type, which is not supported in data flow.
