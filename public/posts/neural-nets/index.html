<!doctype html>





































<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
  dir="ltr"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    
    <title>Neural Networks - Stories and Cheat-Sheets</title>

    
    <meta name="theme-color" />

    
    
    
    
    <meta name="description" content="I&rsquo;m filling in some gaps from my masters and one of the side-quests
that I never completed was coding up a neural network from scratch.
These models have the reputation of being a &lsquo;black box&rsquo; where
data goes in and predictions magically come out. Building one up
takes away this feeling and reminds you that they&rsquo;re just a clever
system of linear algebra cogs.
This post develops a single hidden layer perceptron in OOP style.
It is obviously not supposed to be very good, but three demonstrations
after the model class show that it definitely works." />
    <meta name="author" content="Stories and Cheat-Sheets" />
    

    
    
    
    
    
    
    <link rel="preload stylesheet" as="style" href="https://hlud6646.surge.sh/main.min.css" />

    
    <link
        rel="stylesheet"
        type="text/css"
        href="https://cdn.jsdelivr.net/npm/cm-chessboard@5.2.4/assets/styles/cm-chessboard.css"
    />

    
    
    
    
    
    <link rel="preload" as="image" href="https://hlud6646.surge.sh/theme.png" />

    
    
    
    
    

    
    
    <link rel="preload" as="image" href="https://hlud6646.surge.sh/github.svg" />
    
    <link rel="preload" as="image" href="https://hlud6646.surge.sh/linkedin.svg" />
    
    

    
    
    <script
        defer
        src="https://hlud6646.surge.sh/highlight.min.js"
        onload="hljs.initHighlightingOnLoad();"
    ></script>
    

    
    
    
    
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
  integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
  integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
  integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
  crossorigin="anonymous"
></script>


<script>
  document.addEventListener('DOMContentLoaded', () =>
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
      ],
      
      throwOnError: false,
    }),
  );
</script>

    
    
    

    
    <link
        rel="icon"
        href="https://hlud6646.surge.sh/favicon.ico"
    />
    <link
        rel="apple-touch-icon"
        href="https://hlud6646.surge.sh/apple-touch-icon.png"
    />

    
    <meta name="generator" content="Hugo 0.139.4">

    
    
    
    
    
    
  <meta itemprop="name" content="Neural Networks">
  <meta itemprop="description" content="I’m filling in some gaps from my masters and one of the side-quests that I never completed was coding up a neural network from scratch. These models have the reputation of being a ‘black box’ where data goes in and predictions magically come out. Building one up takes away this feeling and reminds you that they’re just a clever system of linear algebra cogs.
This post develops a single hidden layer perceptron in OOP style. It is obviously not supposed to be very good, but three demonstrations after the model class show that it definitely works.">
  <meta itemprop="datePublished" content="2024-11-06T13:22:54+11:00">
  <meta itemprop="dateModified" content="2024-11-06T13:22:54+11:00">
  <meta itemprop="wordCount" content="1769">
  <meta itemprop="keywords" content="Machine-Learning,Neural-Networks,Python">
    
    <meta property="og:url" content="https://hlud6646.surge.sh/posts/neural-nets/">
  <meta property="og:site_name" content="Stories and Cheat-Sheets">
  <meta property="og:title" content="Neural Networks">
  <meta property="og:description" content="I’m filling in some gaps from my masters and one of the side-quests that I never completed was coding up a neural network from scratch. These models have the reputation of being a ‘black box’ where data goes in and predictions magically come out. Building one up takes away this feeling and reminds you that they’re just a clever system of linear algebra cogs.
This post develops a single hidden layer perceptron in OOP style. It is obviously not supposed to be very good, but three demonstrations after the model class show that it definitely works.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-11-06T13:22:54+11:00">
    <meta property="article:modified_time" content="2024-11-06T13:22:54+11:00">
    <meta property="article:tag" content="Machine-Learning">
    <meta property="article:tag" content="Neural-Networks">
    <meta property="article:tag" content="Python">

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Neural Networks">
  <meta name="twitter:description" content="I’m filling in some gaps from my masters and one of the side-quests that I never completed was coding up a neural network from scratch. These models have the reputation of being a ‘black box’ where data goes in and predictions magically come out. Building one up takes away this feeling and reminds you that they’re just a clever system of linear algebra cogs.
This post develops a single hidden layer perceptron in OOP style. It is obviously not supposed to be very good, but three demonstrations after the model class show that it definitely works.">

    
    

    
    <link rel="canonical" href="https://hlud6646.surge.sh/posts/neural-nets/" />
    
    
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center">
    <div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center">
        <a
            class="-translate-y-[1px] text-2xl font-medium"
            href="https://hlud6646.surge.sh/"
            >Stories and Cheat-Sheets</a
        >
        
        
    </div>

    <div
        class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
        role="button"
        aria-label="Menu"
    ></div>

    

    <script>
        
        const htmlClass = document.documentElement.classList;
        setTimeout(() => {
            htmlClass.remove("not-ready");
        }, 10);

        
        const btnMenu = document.querySelector(".btn-menu");
        btnMenu.addEventListener("click", () => {
            htmlClass.toggle("open");
        });

        
        const metaTheme = document.querySelector('meta[name="theme-color"]');
        const lightBg = "#faf8f1".replace(/"/g, "");
        const setDark = (isDark) => {
            metaTheme.setAttribute("content", isDark ? "#000" : lightBg);
            htmlClass[isDark ? "add" : "remove"]("dark");
            localStorage.setItem("dark", isDark);
        };

        
        const darkScheme = window.matchMedia("(prefers-color-scheme: dark)");
        if (htmlClass.contains("dark")) {
            setDark(true);
        } else {
            const darkVal = localStorage.getItem("dark");
            setDark(darkVal ? darkVal === "true" : darkScheme.matches);
        }

        
        darkScheme.addEventListener("change", (event) => {
            setDark(event.matches);
        });

        
        const btnDark = document.querySelector(".btn-dark");
        btnDark.addEventListener("click", () => {
            setDark(localStorage.getItem("dark") !== "true");
        });
    </script>

    <div
        class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
    >
        
        
        <nav
            class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"
        >
            
            <a
                class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
                href="/about/"
                >About</a
            >
            
            <a
                class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
                href="/contact/"
                >Contact</a
            >
            
        </nav>
        

        
        <nav
            class="mt-12 flex justify-center space-x-10 rtl:space-x-reverse dark:invert ltr:lg:ml-14 rtl:lg:mr-14 lg:mt-0 lg:items-center"
        >
            
            <a
                class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
                style="--url: url(./github.svg)"
                href="https://github.com/hlud6646"
                target="_blank"
                rel="me"
            >
                github
            </a>
            
            <a
                class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
                style="--url: url(./linkedin.svg)"
                href="https://linkedin.com/in/hugoludemann"
                target="_blank"
                rel="me"
            >
                linkedin
            </a>
            
        </nav>
        
    </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100vh-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"
    >
      

<article>
    <header class="mb-14">
        <h1 class="!my-0 pb-2.5">Neural Networks</h1>

        
        <div class="text-xs antialiased opacity-60">
            
            <time>Nov 6, 2024</time>
            
            
            
            
        </div>
        
    </header>

    
    
    <footer class="mt-12 flex flex-wrap">
         
        <a
            class="mb-1.5 ltr:mr-1.5 rtl:ml-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] dark:bg-white/[8%] dark:hover:bg-white/[12%]"
            href="https://hlud6646.surge.sh/tags/machine-learning"
            >machine-learning</a
        >
         
        <a
            class="mb-1.5 ltr:mr-1.5 rtl:ml-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] dark:bg-white/[8%] dark:hover:bg-white/[12%]"
            href="https://hlud6646.surge.sh/tags/neural-networks"
            >neural-networks</a
        >
         
        <a
            class="mb-1.5 ltr:mr-1.5 rtl:ml-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] dark:bg-white/[8%] dark:hover:bg-white/[12%]"
            href="https://hlud6646.surge.sh/tags/python"
            >python</a
        >
        
    </footer>
    

    <section><p>I&rsquo;m filling in some gaps from my masters and one of the side-quests
that I never completed was coding up a neural network from scratch.
These models have the reputation of being a &lsquo;black box&rsquo; where
data goes in and predictions magically come out. Building one up
takes away this feeling and reminds you that they&rsquo;re just a clever
system of linear algebra cogs.</p>
<p>This post develops a single hidden layer perceptron in OOP style.
It is obviously not supposed to be very good, but three demonstrations
after the model class show that it definitely works.</p>
<p>Some ideas for extensions one day:</p>
<ul>
<li>Everything happens by-sample. By-batch is better and should
just be some matrix multiplication.</li>
<li>Multiple hidden layers, with different activation functions.</li>
<li>Stochastic gradient descent.</li>
<li>Momentum in gradient descent?</li>
<li>Regularization.</li>
</ul>
<h2 id="imports-and-configuration">Imports and configuration</h2>
<p>Nothing very exciting here, except maybe <code>autograd</code>. The back-propagation
algorithm used the derivative of the activation functions on each layer.
We could hardcode $\tanh$ and its derivative $1 - \tanh^2$ but it&rsquo;s not
complicated to the activations as parameters and let <code>autograd</code> find the
derivatives.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> autograd.numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> autograd <span style="color:#f92672">import</span> elementwise_grad <span style="color:#66d9ef">as</span> egrad
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_breast_cancer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> log_loss, mean_squared_error
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Callable, Optional
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>set_palette(<span style="color:#e6db74">&#34;muted&#34;</span>)
</span></span><span style="display:flex;"><span>colors <span style="color:#f92672">=</span> sns<span style="color:#f92672">.</span>color_palette()
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>set_style(<span style="color:#e6db74">&#34;darkgrid&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;figure.facecolor&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;none&#34;</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;figure.edgecolor&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;none&#34;</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;figure.figsize&#34;</span>] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>)
</span></span></code></pre></div><h2 id="activation-functions">Activation functions</h2>
<p>These are the activation functions that are used in the examples that follow
the main code, along with $\tanh$ which is imported from <code>numpy</code>.
The modification to <code>softmax</code> was the computer&rsquo;s suggestion and is apparently
useful for avoiding over/underflow errors.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(z):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>z))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax</span>(x):
</span></span><span style="display:flex;"><span>    exp_x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(x <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>max(x))  <span style="color:#75715e"># Subtract max for numerical stability</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> exp_x <span style="color:#f92672">/</span> exp_x<span style="color:#f92672">.</span>sum()
</span></span></code></pre></div><h2 id="model">Model</h2>
<p>The <code>Model</code> class starts with a basic constructor that just records
the parameters it receives and takes a few derivatives.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Model</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        hidden_size: int,
</span></span><span style="display:flex;"><span>        learning_rate: float <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>,
</span></span><span style="display:flex;"><span>        batch_size: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>,
</span></span><span style="display:flex;"><span>        hidden_activation: Callable <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>tanh,
</span></span><span style="display:flex;"><span>        output_activation: Callable <span style="color:#f92672">=</span> sigmoid,
</span></span><span style="display:flex;"><span>        scoring: Optional[Callable] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    ):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Initialize neural network model.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            hidden_size: Number of neurons in hidden layer
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            learning_rate: Learning rate for gradient descent
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            batch_size: Number of samples per batch
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            hidden_activation: Activation function for hidden layer
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            output_activation: Activation function for output layer
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            scoring: Optional scoring function for tracking progress
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hidden_size <span style="color:#f92672">=</span> hidden_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lr <span style="color:#f92672">=</span> learning_rate
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>batch_size <span style="color:#f92672">=</span> batch_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>scoring <span style="color:#f92672">=</span> scoring
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hidden_activation <span style="color:#f92672">=</span> hidden_activation
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hidden_activation_grad <span style="color:#f92672">=</span> egrad(hidden_activation)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output_activation <span style="color:#f92672">=</span> output_activation
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output_activation_grad <span style="color:#f92672">=</span> egrad(output_activation)
</span></span></code></pre></div><h2 id="initialise-weights">Initialise Weights</h2>
<p>When the model is fitted, it will infer the input and output shapes
from the training data. Then it can set up the arrays/vectors that
hold the weights and biases.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init_weights_and_biases</span>(self, x, y):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Initialize weights and biases with small random numbers.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        input_size <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        hidden_size <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>hidden_size
</span></span><span style="display:flex;"><span>        output_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> len(y<span style="color:#f92672">.</span>shape) <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">else</span> y<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        rand <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> dims: np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#f92672">*</span>dims) <span style="color:#f92672">/</span> <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>w1 <span style="color:#f92672">=</span> rand((hidden_size, input_size))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b1 <span style="color:#f92672">=</span> rand((hidden_size,))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>w2 <span style="color:#f92672">=</span> rand((output_size, hidden_size))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b2 <span style="color:#f92672">=</span> rand((output_size,))
</span></span></code></pre></div><h2 id="forward-pass">Forward Pass</h2>
<p>This is relatively straightforward, computing weighted inputs and
activations in the hidden and output layers.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Forward pass through the network.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>z1 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>w1 <span style="color:#f92672">@</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b1
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>a1 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>hidden_activation(self<span style="color:#f92672">.</span>z1)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>z2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>w2 <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>a1 <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b2
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>a2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>output_activation(self<span style="color:#f92672">.</span>z2)
</span></span></code></pre></div><h2 id="backward-pass">Backward Pass</h2>
<p>This is more complicated.
Using the quadratic loss, the error in the output layer is given by
$$\delta^2 = (a^2 - y) \odot \sigma_2^{&rsquo;}(z^2)$$ and the error in the hidden
layer by
$$\delta^1 = w^{2T}d^2 \odot \sigma_1^{&rsquo;}(z^1)$$</p>
<p>These errors are calculated and used to evaluate the gradient of the loss
function with respect to the weights and biases.
Note that this is for a single observation $(x_i, y_i)$.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, xi, yi):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Backward pass through the network.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>output_activation <span style="color:#f92672">==</span> softmax:
</span></span><span style="display:flex;"><span>            d2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>a2 <span style="color:#f92672">-</span> yi
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            d2 <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>a2 <span style="color:#f92672">-</span> yi) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>output_activation_grad(self<span style="color:#f92672">.</span>z2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        d1 <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>w2<span style="color:#f92672">.</span>T <span style="color:#f92672">@</span> d2) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>hidden_activation_grad(self<span style="color:#f92672">.</span>z1)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Gradient of loss wrt w1, b1, w2, b2</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>cost_gradients <span style="color:#f92672">=</span> [d1, xi <span style="color:#f92672">*</span> d1[:, <span style="color:#66d9ef">None</span>], d2, self<span style="color:#f92672">.</span>a1 <span style="color:#f92672">*</span> d2[:, <span style="color:#66d9ef">None</span>]]
</span></span></code></pre></div><h2 id="run-a-batch">Run a Batch</h2>
<p>We can now run the algorithm for a batch of $(x_i, y_i)$ pairs.
Using iteration is definitely not the &lsquo;right&rsquo; way to do this, since
the strength of the algorithm is that it can be implemented using lots
of (fast) matrix multiplications. I have found that this version is easy
to read and so will leave this optimisation for another day.</p>
<p>This function has three steps:</p>
<ol>
<li>For each $(x_i, y_i)$ pair, compute the gradient of the cost function
with respect to the parameter groups $b_1, w_1, b_2, w_2$.</li>
<li>Stack all the $b_1$ values into an array, all the $w_1$ values into
another array etc.</li>
<li>Take the mean of each of these arrays and use it to perform a gradient
descent of the parameters.</li>
</ol>
<p>This is not very clear and could definitely be improved.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch</span>(self, x, y):
</span></span><span style="display:flex;"><span>        grads <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> xi, yi <span style="color:#f92672">in</span> zip(x, y):
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>forward(xi)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>backward(xi, yi)
</span></span><span style="display:flex;"><span>            grads<span style="color:#f92672">.</span>append(self<span style="color:#f92672">.</span>cost_gradients)
</span></span><span style="display:flex;"><span>        grads <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>stack(g, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#66d9ef">for</span> g <span style="color:#f92672">in</span> list(zip(<span style="color:#f92672">*</span>grads))]
</span></span><span style="display:flex;"><span>        db1, dw1, db2, dw2 <span style="color:#f92672">=</span> [a<span style="color:#f92672">.</span>mean(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> grads]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Gradient descent.</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b1 <span style="color:#f92672">-=</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> db1
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>w1 <span style="color:#f92672">-=</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> dw1
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b2 <span style="color:#f92672">-=</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> db2
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>w2 <span style="color:#f92672">-=</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> dw2
</span></span></code></pre></div><h2 id="epoch">Epoch</h2>
<p>Training for one epoch is achieved here by splitting the provided data
into a number of batches (that number being a model hyper-parameter)
and calling the function above on each batch.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_epoch</span>(self, x, y):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, len(x), self<span style="color:#f92672">.</span>batch_size):
</span></span><span style="display:flex;"><span>            batch_x <span style="color:#f92672">=</span> x[i : i <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>batch_size]
</span></span><span style="display:flex;"><span>            batch_y <span style="color:#f92672">=</span> y[i : i <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>batch_size]
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>batch(batch_x, batch_y)
</span></span></code></pre></div><h2 id="fit">Fit</h2>
<p>Finally to fit a model, you simply run call <code>run_epoch</code> however many
times you need. There are a few extras, like allowing <code>pandas</code> input
and recording a scoring metric as training happens but essentially this
is just repeating <code>run_batch</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, x, y, n_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(x, pd<span style="color:#f92672">.</span>DataFrame):
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>values
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(y, pd<span style="color:#f92672">.</span>DataFrame) <span style="color:#f92672">or</span> isinstance(y, pd<span style="color:#f92672">.</span>Series):
</span></span><span style="display:flex;"><span>            y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>values
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(y<span style="color:#f92672">.</span>shape) <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>            y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>init_weights_and_biases(x, y)
</span></span><span style="display:flex;"><span>        history <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        start_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(n_epochs):
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>run_epoch(x, y)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>scoring:
</span></span><span style="display:flex;"><span>                history<span style="color:#f92672">.</span>append(self<span style="color:#f92672">.</span>scoring(y, self<span style="color:#f92672">.</span>predict(x)))
</span></span><span style="display:flex;"><span>                elapsed <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time() <span style="color:#f92672">-</span> start_time
</span></span><span style="display:flex;"><span>                print(
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#e6db74">:</span><span style="color:#e6db74">&gt;3d</span><span style="color:#e6db74">}</span><span style="color:#e6db74">  |  Loss: </span><span style="color:#e6db74">{</span>history[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">  |  Time: </span><span style="color:#e6db74">{</span>elapsed<span style="color:#e6db74">:</span><span style="color:#e6db74">.1f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">s&#34;</span>
</span></span><span style="display:flex;"><span>                )
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#e6db74">:</span><span style="color:#e6db74">&gt;3d</span><span style="color:#e6db74">}</span><span style="color:#e6db74">  |  Time: </span><span style="color:#e6db74">{</span>elapsed<span style="color:#e6db74">:</span><span style="color:#e6db74">.1f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">s&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> history <span style="color:#f92672">or</span> <span style="color:#66d9ef">None</span>
</span></span></code></pre></div><h2 id="making-predictions">Making Predictions</h2>
<p>This is nothing more than making a forward pass through the network
using supplied data and collection the output layer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_one</span>(self, xi):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>forward(xi)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>a2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(x, pd<span style="color:#f92672">.</span>DataFrame):
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>values
</span></span><span style="display:flex;"><span>        predictions <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([self<span style="color:#f92672">.</span>predict_one(xi) <span style="color:#66d9ef">for</span> xi <span style="color:#f92672">in</span> x])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> predictions<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>            predictions <span style="color:#f92672">=</span> predictions<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> predictions
</span></span></code></pre></div><h2 id="example-binary-classification">Example: Binary Classification</h2>
<p>The first example tries to determine the malignancy of breast cancers.
This dataset is known to be an easy binary classification problem and
is a good one to start with, since there are only 569 rows and we will
know quickly if it is working or not. For simplicity only 6 of the available
inputs are selected. The model has three hidden neurons and an appropriate
activation for binary classification.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> load_breast_cancer(as_frame<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, return_X_y<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x[
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;mean radius&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;mean texture&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;mean perimeter&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;worst radius&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;worst texture&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;worst perimeter&#34;</span>,
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> (x <span style="color:#f92672">-</span> x<span style="color:#f92672">.</span>mean(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> x<span style="color:#f92672">.</span>std(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m <span style="color:#f92672">=</span> Model(
</span></span><span style="display:flex;"><span>    hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,
</span></span><span style="display:flex;"><span>    learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.05</span>,
</span></span><span style="display:flex;"><span>    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>,
</span></span><span style="display:flex;"><span>    hidden_activation<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>tanh,
</span></span><span style="display:flex;"><span>    output_activation<span style="color:#f92672">=</span>sigmoid,
</span></span><span style="display:flex;"><span>    scoring<span style="color:#f92672">=</span>log_loss,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>fit(x, y, n_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>yhat <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>predict(x)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Accuracy: </span><span style="color:#e6db74">{</span>np<span style="color:#f92672">.</span>mean(y <span style="color:#f92672">==</span> (yhat <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>))<span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>correct <span style="color:#f92672">=</span> y <span style="color:#f92672">==</span> (yhat <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>correct <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(correct, <span style="color:#e6db74">&#34;Correct&#34;</span>, <span style="color:#e6db74">&#34;Incorrect&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig, axs <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(ncols<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>plot(history)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set(title<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Training loss&#34;</span>, xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Epoch&#34;</span>, ylabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Loss&#34;</span>)
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>scatterplot(x<span style="color:#f92672">=</span>range(len(yhat)), y<span style="color:#f92672">=</span>yhat, hue<span style="color:#f92672">=</span>correct, style<span style="color:#f92672">=</span>correct, s<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;center right&#34;</span>)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set(title<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Hit or Miss&#34;</span>, xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Index&#34;</span>, ylabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Prediction&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>savefig(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#34;out&#34;</span>, <span style="color:#e6db74">&#34;breast_cancer_classification.svg&#34;</span>))
</span></span></code></pre></div><p>Below we can see the training loss decreasing nicely and a good separation
between the two classes. After 50 epochs this model achieves an accuracy of
93%. This is not impressive for this dataset but it works!</p>
<div align="center">
<p><img src="images/breast_cancer_classification.png" alt="Breast cancer classification results"></p>
</div>
<h2 id="example-multiclass-classification">Example: Multiclass Classification</h2>
<p>The model can also handle a multiclass classification problem,
with the only change needed being the output activation which
is a <code>softmax</code> here in place of the earlier <code>sigmoid</code>.
The problem now is to use the length, width and ratio of these
to try and classify a fish.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#34;data&#34;</span>, <span style="color:#e6db74">&#34;fish_data.csv&#34;</span>))
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>sample(frac<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)<span style="color:#f92672">.</span>reset_index(drop<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>drop(columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;species&#34;</span>]), df[<span style="color:#e6db74">&#34;species&#34;</span>]
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>get_dummies(y)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> (x <span style="color:#f92672">-</span> x<span style="color:#f92672">.</span>mean(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> x<span style="color:#f92672">.</span>std(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m <span style="color:#f92672">=</span> Model(
</span></span><span style="display:flex;"><span>    hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
</span></span><span style="display:flex;"><span>    learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>,
</span></span><span style="display:flex;"><span>    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>,
</span></span><span style="display:flex;"><span>    hidden_activation<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>tanh,
</span></span><span style="display:flex;"><span>    output_activation<span style="color:#f92672">=</span>softmax,
</span></span><span style="display:flex;"><span>    scoring<span style="color:#f92672">=</span>log_loss,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>fit(x, y, n_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>yhat <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(m<span style="color:#f92672">.</span>predict(x), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>yv <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(y, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Accuracy: </span><span style="color:#e6db74">{</span>np<span style="color:#f92672">.</span>mean(yhat <span style="color:#f92672">==</span> yv)<span style="color:#e6db74">:</span><span style="color:#e6db74">.2%</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>heatmap(pd<span style="color:#f92672">.</span>crosstab(yhat, yv), annot<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, fmt<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;d&#34;</span>, cbar<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set(xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Predicted&#34;</span>, ylabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Actual&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Use the column names from x as labels.</span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_xticklabels(y<span style="color:#f92672">.</span>columns, rotation<span style="color:#f92672">=</span><span style="color:#ae81ff">90</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set_yticklabels(y<span style="color:#f92672">.</span>columns, rotation<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># plt.show()</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>savefig(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#34;out&#34;</span>, <span style="color:#e6db74">&#34;fish_classification.png&#34;</span>))
</span></span></code></pre></div><p>In the following confusion matrix we can see that most species
are correctly classified, with the exception of <em>Setipinna taty</em> which is
misclassified as  <em>Otolithoides biauritus</em> half the time.</p>
<div align="center">
<p><img src="images/fish_classification.png" alt="Fish classification confusion matrix"></p>
</div>
<h2 id="example-regression">Example: Regression</h2>
<p>Lastly we will try and predict the value of a car, knowing the
year, mileage, miles-per-gallon and engine size.
This dataset is a lot larger than the other two, so we only train for
5 epochs but we can still get reasonable results, enough to believe
that the model is healthy anyway.
The appropriate activation here is the identity function.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#34;data&#34;</span>, <span style="color:#e6db74">&#34;CarsData.csv&#34;</span>))
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>sample(frac<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)<span style="color:#f92672">.</span>reset_index(drop<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> df[[<span style="color:#e6db74">&#34;year&#34;</span>, <span style="color:#e6db74">&#34;mileage&#34;</span>, <span style="color:#e6db74">&#34;mpg&#34;</span>, <span style="color:#e6db74">&#34;engineSize&#34;</span>]]
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> (x <span style="color:#f92672">-</span> x<span style="color:#f92672">.</span>mean(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> x<span style="color:#f92672">.</span>std(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> (df[<span style="color:#e6db74">&#34;price&#34;</span>] <span style="color:#f92672">-</span> df[<span style="color:#e6db74">&#34;price&#34;</span>]<span style="color:#f92672">.</span>mean()) <span style="color:#f92672">/</span> df[<span style="color:#e6db74">&#34;price&#34;</span>]<span style="color:#f92672">.</span>std()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m <span style="color:#f92672">=</span> Model(
</span></span><span style="display:flex;"><span>    hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">6</span>,
</span></span><span style="display:flex;"><span>    learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.05</span>,
</span></span><span style="display:flex;"><span>    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>    output_activation<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x,
</span></span><span style="display:flex;"><span>    scoring<span style="color:#f92672">=</span>mean_squared_error,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>fit(x, y, n_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>indices <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(len(x), <span style="color:#ae81ff">2000</span>)
</span></span><span style="display:flex;"><span>x_sample <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>iloc[indices]
</span></span><span style="display:flex;"><span>y_sample <span style="color:#f92672">=</span> y[indices]
</span></span><span style="display:flex;"><span>y_hat_sample <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>predict(x_sample)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>scatter(y_sample, y_hat_sample, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Predicted&#34;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set(xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Actual&#34;</span>, ylabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Predicted&#34;</span>, xlim<span style="color:#f92672">=</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>), ylim<span style="color:#f92672">=</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add the diagonal line.</span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>plot([<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>], [<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;black&#34;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># plt.show()</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>savefig(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#34;out&#34;</span>, <span style="color:#e6db74">&#34;cars_regression.png&#34;</span>))
</span></span></code></pre></div><p>We can see that points in the predicted vs actual price scatterplot
are clustered around the diagonal, implying the model has a degree of
understanding of the problem.</p>
<div align="center">
<p><img src="images/cars_regression.png" alt="Car price regression results"></p>
</div>
<h2 id="conclusion">Conclusion</h2>
<p>In this post we built a neural network with a single hidden network.
It&rsquo;s built for friendliness not speed, but it works.
By changing the activation function on the output layer the model can
be used for binary/multiclass classification and regression problems.</p>
<p>The next thing I would try is to vectorise the operations inside the
training loop, not because this would be more performant (although it
would) but rather because I think it would actually be easier to read.
Another avenue would be to do this again without OOP. The model doesn&rsquo;t
interact with any other objects, its mostly a container for all its
methods.</p>
<p>All in all though this has been a hard but rewarding project and I do
feel like I understand the back-propagation algorithm a lot better.</p>
<h2 id="references">References:</h2>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">http://neuralnetworksanddeeplearning.com/chap2.html</a></li>
</ul>
</section>

    
    
    
    
    <nav
        class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg !leading-[1.2] *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"
    >
        
        <a class="ltr:pr-3 rtl:pl-3" href="https://hlud6646.surge.sh/posts/sql_project/"
            ><span class="ltr:mr-1.5 rtl:ml-1.5">←</span
            ><span>Car Accidents in the USA</span></a
        >
        
        
        <a
            class="ltr:ml-auto rtl:mr-auto justify-end pl-3"
            href="https://hlud6646.surge.sh/posts/vectorised/"
            ><span>Vectorised Operations</span
            ><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a
        >
        
    </nav>
    
    

    
    

    
    

    


    
</article>


    </main>

    <footer
    class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"
>
    <div class="mr-auto">
         &copy; 2025
        <a class="link" href="https://hlud6646.surge.sh/">Stories and Cheat-Sheets</a>
        
    </div>
    
</footer>

  </body>
</html>
