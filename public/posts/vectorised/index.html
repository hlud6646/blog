<!doctype html>





































<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
  dir="ltr"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    
    <title>Vectorised Operations - Stories and Cheat-Sheets</title>

    
    <meta name="theme-color" />

    
    
    
    
    <meta name="description" content="It&rsquo;s well known that vectorised operations should be preferred in numpy/pandas
for reasons of performance. A less common reason is that it actually makes the
code easier to read. For a simple example imagine that you are doing a multivariate
linear regression to transform an $n$ dimensional input to an $m$ dimensional output.
The model takes the form $y = Bx$ where $B$ is an $(n \times m)$ matrix.
Now imagine you have $k$ pairs of $(x, y)$ data, and the $k$ inputs are collected
in an iterable data structure $X$. The naive way to gather model
predictions is to code up a single prediction and then loop over all data:" />
    <meta name="author" content="Stories and Cheat-Sheets" />
    

    
    
    
    
    
    
    <link rel="preload stylesheet" as="style" href="https://hlud6646.surge.sh/main.min.css" />

    
    <link
        rel="stylesheet"
        type="text/css"
        href="https://cdn.jsdelivr.net/npm/cm-chessboard@5.2.4/assets/styles/cm-chessboard.css"
    />

    
    
    
    
    
    <link rel="preload" as="image" href="https://hlud6646.surge.sh/theme.png" />

    
    
    
    
    

    
    
    <link rel="preload" as="image" href="https://hlud6646.surge.sh/github.svg" />
    
    <link rel="preload" as="image" href="https://hlud6646.surge.sh/linkedin.svg" />
    
    

    
    
    <script
        defer
        src="https://hlud6646.surge.sh/highlight.min.js"
        onload="hljs.initHighlightingOnLoad();"
    ></script>
    

    
    
    
    
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
  integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
  integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
  integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
  crossorigin="anonymous"
></script>


<script>
  document.addEventListener('DOMContentLoaded', () =>
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
      ],
      
      throwOnError: false,
    }),
  );
</script>

    
    
    

    
    <link
        rel="icon"
        href="https://hlud6646.surge.sh/favicon.ico"
    />
    <link
        rel="apple-touch-icon"
        href="https://hlud6646.surge.sh/apple-touch-icon.png"
    />

    
    <meta name="generator" content="Hugo 0.139.4">

    
    
    
    
    
    
  <meta itemprop="name" content="Vectorised Operations">
  <meta itemprop="description" content="It’s well known that vectorised operations should be preferred in numpy/pandas for reasons of performance. A less common reason is that it actually makes the code easier to read. For a simple example imagine that you are doing a multivariate linear regression to transform an $n$ dimensional input to an $m$ dimensional output. The model takes the form $y = Bx$ where $B$ is an $(n \times m)$ matrix. Now imagine you have $k$ pairs of $(x, y)$ data, and the $k$ inputs are collected in an iterable data structure $X$. The naive way to gather model predictions is to code up a single prediction and then loop over all data:">
  <meta itemprop="datePublished" content="2024-10-09T08:01:10+11:00">
  <meta itemprop="dateModified" content="2024-10-09T08:01:10+11:00">
  <meta itemprop="wordCount" content="262">
  <meta itemprop="keywords" content="Python,Numpy">
    
    <meta property="og:url" content="https://hlud6646.surge.sh/posts/vectorised/">
  <meta property="og:site_name" content="Stories and Cheat-Sheets">
  <meta property="og:title" content="Vectorised Operations">
  <meta property="og:description" content="It’s well known that vectorised operations should be preferred in numpy/pandas for reasons of performance. A less common reason is that it actually makes the code easier to read. For a simple example imagine that you are doing a multivariate linear regression to transform an $n$ dimensional input to an $m$ dimensional output. The model takes the form $y = Bx$ where $B$ is an $(n \times m)$ matrix. Now imagine you have $k$ pairs of $(x, y)$ data, and the $k$ inputs are collected in an iterable data structure $X$. The naive way to gather model predictions is to code up a single prediction and then loop over all data:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-10-09T08:01:10+11:00">
    <meta property="article:modified_time" content="2024-10-09T08:01:10+11:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="Numpy">

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Vectorised Operations">
  <meta name="twitter:description" content="It’s well known that vectorised operations should be preferred in numpy/pandas for reasons of performance. A less common reason is that it actually makes the code easier to read. For a simple example imagine that you are doing a multivariate linear regression to transform an $n$ dimensional input to an $m$ dimensional output. The model takes the form $y = Bx$ where $B$ is an $(n \times m)$ matrix. Now imagine you have $k$ pairs of $(x, y)$ data, and the $k$ inputs are collected in an iterable data structure $X$. The naive way to gather model predictions is to code up a single prediction and then loop over all data:">

    
    

    
    <link rel="canonical" href="https://hlud6646.surge.sh/posts/vectorised/" />
    
    
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center">
    <div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center">
        <a
            class="-translate-y-[1px] text-2xl font-medium"
            href="https://hlud6646.surge.sh/"
            >Stories and Cheat-Sheets</a
        >
        
        
    </div>

    <div
        class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
        role="button"
        aria-label="Menu"
    ></div>

    

    <script>
        
        const htmlClass = document.documentElement.classList;
        setTimeout(() => {
            htmlClass.remove("not-ready");
        }, 10);

        
        const btnMenu = document.querySelector(".btn-menu");
        btnMenu.addEventListener("click", () => {
            htmlClass.toggle("open");
        });

        
        const metaTheme = document.querySelector('meta[name="theme-color"]');
        const lightBg = "#faf8f1".replace(/"/g, "");
        const setDark = (isDark) => {
            metaTheme.setAttribute("content", isDark ? "#000" : lightBg);
            htmlClass[isDark ? "add" : "remove"]("dark");
            localStorage.setItem("dark", isDark);
        };

        
        const darkScheme = window.matchMedia("(prefers-color-scheme: dark)");
        if (htmlClass.contains("dark")) {
            setDark(true);
        } else {
            const darkVal = localStorage.getItem("dark");
            setDark(darkVal ? darkVal === "true" : darkScheme.matches);
        }

        
        darkScheme.addEventListener("change", (event) => {
            setDark(event.matches);
        });

        
        const btnDark = document.querySelector(".btn-dark");
        btnDark.addEventListener("click", () => {
            setDark(localStorage.getItem("dark") !== "true");
        });
    </script>

    <div
        class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
    >
        
        
        <nav
            class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"
        >
            
            <a
                class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
                href="/about/"
                >About</a
            >
            
            <a
                class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
                href="/contact/"
                >Contact</a
            >
            
        </nav>
        

        
        <nav
            class="mt-12 flex justify-center space-x-10 rtl:space-x-reverse dark:invert ltr:lg:ml-14 rtl:lg:mr-14 lg:mt-0 lg:items-center"
        >
            
            <a
                class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
                style="--url: url(./github.svg)"
                href="https://github.com/hlud6646"
                target="_blank"
                rel="me"
            >
                github
            </a>
            
            <a
                class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
                style="--url: url(./linkedin.svg)"
                href="https://linkedin.com/in/hugoludemann"
                target="_blank"
                rel="me"
            >
                linkedin
            </a>
            
        </nav>
        
    </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100vh-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"
    >
      

<article>
    <header class="mb-14">
        <h1 class="!my-0 pb-2.5">Vectorised Operations</h1>

        
        <div class="text-xs antialiased opacity-60">
            
            <time>Oct 9, 2024</time>
            
            
            
            
        </div>
        
    </header>

    
    
    <footer class="mt-12 flex flex-wrap">
         
        <a
            class="mb-1.5 ltr:mr-1.5 rtl:ml-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] dark:bg-white/[8%] dark:hover:bg-white/[12%]"
            href="https://hlud6646.surge.sh/tags/python"
            >python</a
        >
         
        <a
            class="mb-1.5 ltr:mr-1.5 rtl:ml-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] dark:bg-white/[8%] dark:hover:bg-white/[12%]"
            href="https://hlud6646.surge.sh/tags/numpy"
            >numpy</a
        >
        
    </footer>
    

    <section><p>It&rsquo;s well known that vectorised operations should be preferred in numpy/pandas
for reasons of performance. A less common reason is that it actually makes the
code easier to read. For a simple example imagine that you are doing a multivariate
linear regression to transform an $n$ dimensional input to an $m$ dimensional output.
The model takes the form $y = Bx$ where $B$ is an $(n \times m)$ matrix.
Now imagine you have $k$ pairs of $(x, y)$ data, and the $k$ inputs are collected
in an iterable data structure $X$. The naive way to gather model
predictions is to code up a single prediction and then loop over all data:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Single observation.</span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> x <span style="color:#f92672">@</span> B
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># All observations.</span>
</span></span><span style="display:flex;"><span>Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([x <span style="color:#f92672">@</span> B <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> X])
</span></span></code></pre></div><p>Everybody will tell you this is bad because you are using slow loops, and they&rsquo;re
right.
But consider an alternative reason why the vectorised version would be better.
If $X$ is a $(k \times n)$ array holding your input data, you can simply write</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># All observations.</span>
</span></span><span style="display:flex;"><span>Y <span style="color:#f92672">=</span> X <span style="color:#f92672">@</span> B
</span></span></code></pre></div><p>and you will notice that apart from capitalisation the form of this equation is
identical to the prediction for a single observation.
In other words, if your arrays have the right shape, then you can almost forget
that you are operating on all your data at once and use a mental model of
manipulating a single observation. The vectorised version is so clear and saves
the reader having to go and examine the contents of a loop or comprehension.</p>
</section>

    
    
    
    
    <nav
        class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg !leading-[1.2] *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"
    >
        
        <a class="ltr:pr-3 rtl:pl-3" href="https://hlud6646.surge.sh/posts/neural-nets/"
            ><span class="ltr:mr-1.5 rtl:ml-1.5">←</span
            ><span>Neural Networks</span></a
        >
        
        
        <a
            class="ltr:ml-auto rtl:mr-auto justify-end pl-3"
            href="https://hlud6646.surge.sh/posts/building_grep/"
            ><span>Rebuilding Grep</span
            ><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a
        >
        
    </nav>
    
    

    
    

    
    

    


    
</article>


    </main>

    <footer
    class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"
>
    <div class="mr-auto">
         &copy; 2025
        <a class="link" href="https://hlud6646.surge.sh/">Stories and Cheat-Sheets</a>
        
    </div>
    
</footer>

  </body>
</html>
